{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python2\n",
    "# -*- coding: utf-8 -*-\n",
    "import pprint\n",
    "import pymongo\n",
    "import datetime\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from scipy import spatial\n",
    "from scipy.sparse import isspmatrix, dok_matrix, csc_matrix\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import pylab\n",
    "\n",
    "import os\n",
    "\n",
    "import progressbar\n",
    "\n",
    "db_name = 'twitter'\n",
    "col_name = 'after_process_replab'\n",
    "\n",
    "col_output_score = 'cluster_score'\n",
    "col_output_event = 'event_list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN, AffinityPropagation, MeanShift, estimate_bandwidth\n",
    "# from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "\n",
    "from igraph import *\n",
    "import igraph\n",
    "import math\n",
    "from operator import itemgetter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_midnight(time):\n",
    "    return time.replace(minute=0, hour=0, second=0, microsecond=0)\n",
    "\n",
    "def get_time_gap(time,hour_gap=1,min_gap=1):\n",
    "    h = time.hour\n",
    "    m = time.minute\n",
    "    o_h = h/hour_gap*hour_gap\n",
    "    o_m = m/min_gap*min_gap\n",
    "    return time.replace(hour=o_h, minute=o_m, second=0, microsecond=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim(doc_a,doc_b):\n",
    "    return 1 - spatial.distance.cosine(doc_a, doc_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    client = MongoClient()\n",
    "    db = client[db_name]\n",
    "\n",
    "    result = db[col_name].create_index([('ts', pymongo.ASCENDING)])\n",
    "    cursor = db[col_name].find({ 'ts':{'$gte':1338508800},'topic_dense':{'$in':[0,1]} }).sort([('ts', pymongo.ASCENDING)])\n",
    "#     ,'topic_dense':{'$in':[0,1]}\n",
    "\n",
    "    date = []\n",
    "    text = []\n",
    "    all_doc = []\n",
    "\n",
    "    bar = progressbar.ProgressBar(maxval=cursor.count()+1, widgets=[progressbar.Bar('#', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    data=1\n",
    "    for doc in cursor:\n",
    "        data += 1\n",
    "        bar.update(data)\n",
    "        \n",
    "        ts = doc['ts']\n",
    "        datetime_object = datetime.datetime.fromtimestamp(ts)\n",
    "        if True:\n",
    "#         if len(doc['hashtags']) != 0:\n",
    "            buf = doc['nouns_nltk']\n",
    "            for x in doc['hashtags']:\n",
    "                if not x.lower() in buf:\n",
    "                    buf.append(x.lower())\n",
    "            all_doc.append(doc)\n",
    "            date.append(datetime_object)\n",
    "            text.append(' '.join(buf))\n",
    "    bar.finish()\n",
    "#     print len(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_group(n_cluster, all_doc, doc_key, g_id):\n",
    "        group = []\n",
    "        for i in range(n_cluster):\n",
    "            group.append([])\n",
    "        \n",
    "        for i in range( len(g_id) ):\n",
    "            group[ g_id[i] ].append(all_doc[i][doc_key])\n",
    "        return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_day(date,text,all_doc):\n",
    "    s_text = {}\n",
    "    s_all_doc = {}\n",
    "    for datetime_object in date:\n",
    "        i = date.index(datetime_object)\n",
    "        key = get_midnight(datetime_object)\n",
    "        if not s_text.has_key(key):\n",
    "            s_text[key] = []\n",
    "            s_all_doc[key] = []\n",
    "        s_text[key].append(text[i])\n",
    "        s_all_doc[key].append(all_doc[i])\n",
    "    return s_text,s_all_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_text,s_all_doc = split_day(date,text,all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dbscan(test):\n",
    "    # Compute DBSCAN\n",
    "    cluster = DBSCAN(eps=0.3, min_samples=3,metric='precomputed').fit(test)\n",
    "    # distance <= eps\n",
    "    core_samples_mask = np.zeros_like(cluster.labels_, dtype=bool)\n",
    "    core_samples_mask[cluster.core_sample_indices_] = True\n",
    "    labels = cluster.labels_\n",
    "    \n",
    "#     print cluster.core_sample_indices_\n",
    "\n",
    "#     print labels\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "#     print('Estimated number of clusters: %d' % n_clusters_)\n",
    "#     if n_clusters_>1:\n",
    "#         print(\"Silhouette Coefficient: %0.3f\"\n",
    "#           % metrics.silhouette_score(test, labels))\n",
    "        \n",
    "    group = []\n",
    "    for i in range(n_clusters_+1):\n",
    "        group.append([])\n",
    "    index = 0\n",
    "    for i in labels:\n",
    "        group[i].append(index)\n",
    "        index += 1\n",
    "        \n",
    "    re_labels = -1\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == -1:\n",
    "            labels[i] = re_labels\n",
    "            re_labels -= 1\n",
    "    \n",
    "    group = group[:-1]\n",
    "    # for i in range(n_clusters_):\n",
    "    #     print len(group[i])\n",
    "\n",
    "    return group, labels, n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AffinityPropagation\n",
    "def aff_cluster(test):\n",
    "    cluster = AffinityPropagation(affinity='euclidean').fit(test)\n",
    "    cluster_centers = cluster.cluster_centers_indices_\n",
    "    labels = cluster.labels_\n",
    "#     print cluster_centers_indices\n",
    "#     print labels\n",
    "\n",
    "    # print labels\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels))\n",
    "#     print('Estimated number of clusters: %d' % n_clusters_)\n",
    "        \n",
    "    group = []\n",
    "    for i in range(n_clusters_):\n",
    "        group.append([])\n",
    "    index = 0\n",
    "    for i in labels:\n",
    "        group[i].append(index)\n",
    "        index += 1\n",
    "        \n",
    "    regroup = []\n",
    "    relabel = []\n",
    "    recenters = []\n",
    "    index = 0\n",
    "    for mem in group:\n",
    "        if len(mem) >= 3:\n",
    "            regroup.append(mem)\n",
    "            relabel.append(index)\n",
    "            recenters.append(cluster_centers[index])\n",
    "#             print index\n",
    "        index += 1\n",
    "    \n",
    "    new_label = []\n",
    "    for i in labels:\n",
    "        if i in relabel:\n",
    "            new_label.append(relabel.index(i))\n",
    "        else:\n",
    "            new_label.append(-1)\n",
    "    # for i in range(n_clusters_):\n",
    "    #     print len(group[i])\n",
    "#     print new_label\n",
    "    new_n_clusters_ = len(set(new_label)) - (1 if -1 in labels else 0)\n",
    "#     print('Estimated number of clusters: %d' % n_clusters_)\n",
    "\n",
    "    return group, labels, n_clusters_, cluster_centers, regroup, new_label, new_n_clusters_, recenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import markov_clustering as mc\n",
    "# py 2 not have\n",
    "def sparse_allclose(a, b, rtol=1e-5, atol=1e-8):\n",
    "    \"\"\"\n",
    "    Version of np.allclose for use with sparse matrices\n",
    "    \"\"\"\n",
    "    c = np.abs(a - b) - rtol * np.abs(b)\n",
    "    # noinspection PyUnresolvedReferences\n",
    "    return c.max() <= atol\n",
    "\n",
    "\n",
    "def normalize(matrix):\n",
    "    \"\"\"\n",
    "    Normalize the columns of the given matrix\n",
    "    \n",
    "    :param matrix: The matrix to be normalized\n",
    "    :returns: The normalized matrix\n",
    "    \"\"\"\n",
    "    return sklearn.preprocessing.normalize(matrix, norm=\"l1\", axis=0)\n",
    "\n",
    "\n",
    "def inflate(matrix, power):\n",
    "    \"\"\"\n",
    "    Apply cluster inflation to the given matrix by raising\n",
    "    each element to the given power.\n",
    "    \n",
    "    :param matrix: The matrix to be inflated\n",
    "    :param power: Cluster inflation parameter\n",
    "    :returns: The inflated matrix\n",
    "    \"\"\"\n",
    "    if isspmatrix(matrix):\n",
    "        return normalize(matrix.power(power))\n",
    "\n",
    "    return normalize(np.power(matrix, power))\n",
    "\n",
    "\n",
    "def expand(matrix, power):\n",
    "    \"\"\"\n",
    "    Apply cluster expansion to the given matrix by raising\n",
    "    the matrix to the given power.\n",
    "    \n",
    "    :param matrix: The matrix to be expanded\n",
    "    :param power: Cluster expansion parameter\n",
    "    :returns: The expanded matrix\n",
    "    \"\"\"\n",
    "    if isspmatrix(matrix):\n",
    "        return matrix ** power\n",
    "\n",
    "    return np.linalg.matrix_power(matrix, power)\n",
    "\n",
    "\n",
    "def add_self_loops(matrix, loop_value):\n",
    "    \"\"\"\n",
    "    Add self-loops to the matrix by setting the diagonal\n",
    "    to loop_value\n",
    "    \n",
    "    :param matrix: The matrix to add loops to\n",
    "    :param loop_value: Value to use for self-loops\n",
    "    :returns: The matrix with self-loops\n",
    "    \"\"\"\n",
    "    shape = matrix.shape\n",
    "    assert shape[0] == shape[1], \"Error, matrix is not square\"\n",
    "\n",
    "    if isspmatrix(matrix):\n",
    "        new_matrix = matrix.todok()\n",
    "    else:\n",
    "        new_matrix = matrix.copy()\n",
    "\n",
    "    for i in range(shape[0]):\n",
    "        new_matrix[i, i] = loop_value\n",
    "\n",
    "    if isspmatrix(matrix):\n",
    "        return new_matrix.tocsc()\n",
    "\n",
    "    return new_matrix\n",
    "\n",
    "\n",
    "def prune(matrix, threshold):\n",
    "    \"\"\"\n",
    "    Prune the matrix so that very small edges are removed\n",
    "    \n",
    "    :param matrix: The matrix to be pruned\n",
    "    :param threshold: The value below which edges will be removed\n",
    "    :returns: The pruned matrix\n",
    "    \"\"\"\n",
    "    if isspmatrix(matrix):\n",
    "        pruned = dok_matrix(matrix.shape)\n",
    "        pruned[matrix >= threshold] = matrix[matrix >= threshold]\n",
    "        pruned = pruned.tocsc()\n",
    "    else:\n",
    "        pruned = matrix.copy()\n",
    "        pruned[pruned < threshold] = 0\n",
    "\n",
    "    return pruned\n",
    "\n",
    "\n",
    "def converged(matrix1, matrix2):\n",
    "    \"\"\"\n",
    "    Check for convergence by determining if \n",
    "    matrix1 and matrix2 are approximately equal.\n",
    "    \n",
    "    :param matrix1: The matrix to compare with matrix2\n",
    "    :param matrix2: The matrix to compare with matrix1\n",
    "    :returns: True if matrix1 and matrix2 approximately equal\n",
    "    \"\"\"\n",
    "    if isspmatrix(matrix1) or isspmatrix(matrix2):\n",
    "        return sparse_allclose(matrix1, matrix2)\n",
    "\n",
    "    return np.allclose(matrix1, matrix2)\n",
    "\n",
    "\n",
    "def iterate(matrix, expansion, inflation):\n",
    "    \"\"\"\n",
    "    Run a single iteration (expansion + inflation) of the mcl algorithm\n",
    "    \n",
    "    :param matrix: The matrix to perform the iteration on\n",
    "    :param expansion: Cluster expansion factor\n",
    "    :param inflation: Cluster inflation factor\n",
    "    \"\"\"\n",
    "    # Expansion\n",
    "    matrix = expand(matrix, expansion)\n",
    "\n",
    "    # Inflation\n",
    "    matrix = inflate(matrix, inflation)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def get_clusters(matrix):\n",
    "    \"\"\"\n",
    "    Retrieve the clusters from the matrix\n",
    "    \n",
    "    :param matrix: The matrix produced by the MCL algorithm\n",
    "    :returns: A list of tuples where each tuple represents a cluster and\n",
    "              contains the indices of the nodes belonging to the cluster\n",
    "    \"\"\"\n",
    "    if not isspmatrix(matrix):\n",
    "        # cast to sparse so that we don't need to handle different \n",
    "        # matrix types\n",
    "        matrix = csc_matrix(matrix)\n",
    "\n",
    "    # get the attractors - non-zero elements of the matrix diagonal\n",
    "    attractors = matrix.diagonal().nonzero()[0]\n",
    "\n",
    "    # somewhere to put the clusters\n",
    "    clusters = set()\n",
    "\n",
    "    # the nodes in the same row as each attractor form a cluster\n",
    "    for attractor in attractors:\n",
    "        cluster = tuple(matrix.getrow(attractor).nonzero()[1].tolist())\n",
    "        clusters.add(cluster)\n",
    "\n",
    "    return sorted(list(clusters))\n",
    "\n",
    "\n",
    "def run_mcl(matrix, expansion=2, inflation=2, loop_value=1,\n",
    "            iterations=100, pruning_threshold=0.001, pruning_frequency=1,\n",
    "            convergence_check_frequency=1, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform MCL on the given similarity matrix\n",
    "    \n",
    "    :param matrix: The similarity matrix to cluster\n",
    "    :param expansion: The cluster expansion factor\n",
    "    :param inflation: The cluster inflation factor\n",
    "    :param loop_value: Initialization value for self-loops\n",
    "    :param iterations: Maximum number of iterations\n",
    "           (actual number of iterations will be less if convergence is reached)\n",
    "    :param pruning_threshold: Threshold below which matrix elements will be set\n",
    "           set to 0\n",
    "    :param pruning_frequency: Perform pruning every 'pruning_frequency'\n",
    "           iterations. \n",
    "    :param convergence_check_frequency: Perform the check for convergence\n",
    "           every convergence_check_frequency iterations\n",
    "    :param verbose: Print extra information to the console\n",
    "    :returns: The final matrix\n",
    "    \"\"\"\n",
    "#     print(\"-\" * 50)\n",
    "#     print(\"MCL Parameters\")\n",
    "#     print(\"Expansion: {}\".format(expansion))\n",
    "#     print(\"Inflation: {}\".format(inflation))\n",
    "#     if pruning_threshold > 0:\n",
    "#         print(\"Pruning threshold: {}, frequency: {} iteration{}\".format(\n",
    "#             pruning_threshold, pruning_frequency, \"s\" if pruning_frequency > 1 else \"\"))\n",
    "#     else:\n",
    "#         print(\"No pruning\")\n",
    "#     print(\"Convergence check: {} iteration{}\".format(\n",
    "#         convergence_check_frequency, \"s\" if convergence_check_frequency > 1 else \"\"))\n",
    "#     print(\"Maximum iterations: {}\".format(iterations))\n",
    "#     print(\"{} matrix mode\".format(\"Sparse\" if isspmatrix(matrix) else \"Dense\"))\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "    # Initialize self-loops\n",
    "    if loop_value > 0:\n",
    "        matrix = add_self_loops(matrix, loop_value)\n",
    "\n",
    "    # Normalize\n",
    "    matrix = normalize(matrix)\n",
    "\n",
    "    # iterations\n",
    "    for i in range(iterations):\n",
    "#         print(\"Iteration {}\".format(i + 1))\n",
    "\n",
    "        # store current matrix for convergence checking\n",
    "        last_mat = matrix.copy()\n",
    "\n",
    "        # perform MCL expansion and inflation\n",
    "        matrix = iterate(matrix, expansion, inflation)\n",
    "\n",
    "        # prune\n",
    "        if pruning_threshold > 0 and i % pruning_frequency == pruning_frequency - 1:\n",
    "#             print(\"Pruning\")\n",
    "            matrix = prune(matrix, pruning_threshold)\n",
    "\n",
    "        # Check for convergence\n",
    "        if i % convergence_check_frequency == convergence_check_frequency - 1:\n",
    "#             print(\"Checking for convergence\")\n",
    "            if converged(matrix, last_mat):\n",
    "#                 print(\"Converged after {} iteration{}\".format(i + 1, \"s\" if i > 0 else \"\"))\n",
    "                break\n",
    "\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for py 3.xx \n",
    "# import markov_clustering as mc\n",
    "\n",
    "# mcl = mc.run_mcl(test,inflation = 1.5)           # run MCL with default parameters\n",
    "# cluster = mc.get_clusters(mcl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_mcl(cluster, n):\n",
    "    labels = [0]*n\n",
    "    filter_labels = [0]*n\n",
    "    i = 0\n",
    "    filter_i = -1\n",
    "    num_filter = 0\n",
    "    filter_cluster = []\n",
    "    for mem in cluster:\n",
    "        if len(mem) <3:\n",
    "            num_filter = -1\n",
    "        else:\n",
    "            filter_i += 1\n",
    "            num_filter = filter_i\n",
    "            filter_cluster.append(mem)\n",
    "        for index in mem:\n",
    "            labels[index] = i\n",
    "            filter_labels[index] = num_filter\n",
    "        i += 1\n",
    "    return labels, filter_labels,filter_cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_true(all_doc):\n",
    "    topic_labels = []\n",
    "    minor_class_labels = []\n",
    "    major_class_labels = []\n",
    "    topic_dense = []\n",
    "    for doc in all_doc:\n",
    "        topic_labels.append(doc['topic'])\n",
    "        minor_class_labels.append(doc['minor_class'])\n",
    "        major_class_labels.append(doc['major_class'])\n",
    "        topic_dense.append(doc['topic_dense'])\n",
    "    return topic_labels,minor_class_labels,major_class_labels,topic_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rend_index(labels_true, labels):\n",
    "    tp_tn = 0 #same cluster in both labels + diff cluster in both labels\n",
    "    n = len(labels_true)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            lt_same = (labels_true[i] == labels_true[j])\n",
    "            l_same = (labels[i] == labels[j])\n",
    "            if lt_same == l_same:\n",
    "                tp_tn += 1\n",
    "    return 1.0*tp_tn/(n*(n-1)/2)\n",
    "\n",
    "def cal_cluster_score(labels_true, labels):\n",
    "    homogeneity, completeness, v_measure  = metrics.homogeneity_completeness_v_measure(labels_true, labels)\n",
    "    acc = rend_index(labels_true, labels)\n",
    "    return homogeneity, completeness, v_measure, acc\n",
    "#     adjusted_rand_score =  metrics.adjusted_rand_score(labels_true, labels)\n",
    "#     adjusted_mutual_info_score = metrics.adjusted_mutual_info_score(labels_true, labels)\n",
    "#     fowlkes_mallows_score = metrics.fowlkes_mallows_score(labels_true, labels)\n",
    "#     return homogeneity, completeness, v_measure, adjusted_rand_score, adjusted_mutual_info_score, fowlkes_mallows_score\n",
    "\n",
    "\n",
    "# Measured in three levels. major_class > minor_class > topic\n",
    "\n",
    "\n",
    "# 1 : use all topic \n",
    "# 2 : use only topic happen > 3 time and not ignore topic in true label and topic that Selected by cluster algorithm \n",
    "\n",
    "def cluster_score(time,levels,description,labels_true, labels,topic_dense):\n",
    "    n = len(labels_true)\n",
    "    #1\n",
    "    homogeneity, completeness, v_measure, acc = cal_cluster_score(labels_true, labels)\n",
    "    \n",
    "    db[col_output_score].update_one({'time':time,'levels':levels,'description':description,'type':1},\n",
    "                              {\"$set\":{'homogeneity_score':homogeneity, \n",
    "                                       'completeness_score':completeness, \n",
    "                                       'v_measure_score':v_measure, \n",
    "                                       'accuracy':acc,\n",
    "#                                        'adjusted_rand_score':adjusted_rand_score, \n",
    "#                                        'adjusted_mutual_info_score':adjusted_mutual_info_score, \n",
    "#                                        'fowlkes_mallows_score':fowlkes_mallows_score\n",
    "                                      }}, upsert=True)\n",
    "    \n",
    "    #2\n",
    "    filter_labels_true = []\n",
    "    filter_labels = []\n",
    "    for i in range(n):\n",
    "        if topic_dense[i] == 1 or labels[i] >= 0:\n",
    "            filter_labels_true.append(labels_true[i])\n",
    "            filter_labels.append(labels[i])\n",
    "    homogeneity, completeness, v_measure, acc = cal_cluster_score(filter_labels_true, filter_labels)\n",
    "    db[col_output_score].update_one({'time':time,'levels':levels,'description':description,'type':2},\n",
    "                      {\"$set\":{'homogeneity_score':homogeneity, \n",
    "                               'completeness_score':completeness, \n",
    "                               'v_measure_score':v_measure, \n",
    "                               'accuracy':acc,\n",
    "#                                'adjusted_rand_score':adjusted_rand_score, \n",
    "#                                'adjusted_mutual_info_score':adjusted_mutual_info_score, \n",
    "#                                'fowlkes_mallows_score':fowlkes_mallows_score\n",
    "                              }}, upsert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_centroid_score(tf,group_member,feature_names,topic_name):\n",
    "    centroid_list = []\n",
    "    centroid_word_list = []\n",
    "    score_list = []\n",
    "    centroid_name_list = []\n",
    "    size_list = []\n",
    "    \n",
    "    for group in group_member:\n",
    "        size_list.append(len(group))\n",
    "        most_sim = -1\n",
    "        most_sim_index = 0\n",
    "        count = np.zeros(len(feature_names))\n",
    "        for member in group:\n",
    "            count += tf[member].toarray()[0]\n",
    "        centroid = count / len(group)\n",
    "        centroid_list.append(centroid)\n",
    "        \n",
    "        centroid_word = []\n",
    "        for word_index in range(len(feature_names)):\n",
    "            if count[word_index] != 0:\n",
    "                word_data = tuple([centroid[word_index] ,feature_names[word_index]])\n",
    "                centroid_word.append(word_data)\n",
    "        centroid_word_list.append( sorted(centroid_word, reverse  = True)[:5])\n",
    "            \n",
    "        sum_sim = 0\n",
    "        for member in group:\n",
    "            m_tf = tf[member].toarray()[0]\n",
    "            sim = get_sim(m_tf,centroid)\n",
    "            sum_sim += sim\n",
    "            \n",
    "            if sim > most_sim:\n",
    "                most_sim_index = member\n",
    "                most_sim = sim\n",
    "        score = sum_sim/len(group)\n",
    "        score_list.append(score)\n",
    "        \n",
    "        centroid_name_list.append(topic_name[most_sim_index])\n",
    "        \n",
    "        \n",
    "    return centroid_list, centroid_word_list, score_list, centroid_name_list, size_list\n",
    "\n",
    "def find_centroid_score_aff(tf,group_member,feature_names,topic_name,centroid_list_index):\n",
    "    centroid_list = []\n",
    "    centroid_word_list = []\n",
    "    score_list = []\n",
    "    centroid_name_list = []\n",
    "    size_list = []\n",
    "    i = 0\n",
    "    for group in group_member:\n",
    "        size_list.append(len(group))\n",
    "        centroid_word = []\n",
    "        centroid = np.zeros(len(feature_names)) + tf[centroid_list_index[i]].toarray()[0]\n",
    "        centroid_list.append(centroid)\n",
    "        \n",
    "        for word_index in range(len(feature_names)):\n",
    "            if centroid[word_index] != 0:\n",
    "                word_data = tuple([centroid[word_index] ,feature_names[word_index]])\n",
    "                centroid_word.append(word_data)\n",
    "        centroid_word_list.append( sorted(centroid_word, reverse  = True)[:5])\n",
    "            \n",
    "        sum_sim = 0\n",
    "        for member in group:\n",
    "            m_tf = tf[member].toarray()[0]\n",
    "            sim = get_sim(m_tf,centroid)\n",
    "            sum_sim += sim\n",
    "        \n",
    "        score = sum_sim/len(group)\n",
    "        score_list.append(score)\n",
    "        \n",
    "        centroid_name_list.append(topic_name[ centroid_list_index[i] ])\n",
    "        \n",
    "        i +=1\n",
    "        \n",
    "        \n",
    "    return centroid_list, centroid_word_list, score_list, centroid_name_list, size_list\n",
    "\n",
    "\n",
    "def write_event(time,description, feature_names, centroid_list, centroid_word_list, score_list, centroid_name_list, size_list,tweet_id_list, author_list):\n",
    "    n = len(centroid_list)\n",
    "    db[col_output_event].update_one({'time':time,'description':'feature_names','group_id':-1},\n",
    "                          {\"$set\":{'feature_names':feature_names, \n",
    "                                  }}, upsert=True)\n",
    "    for i in range(n):\n",
    "        centroid = tuple(centroid_list[i])\n",
    "        name = centroid_word_list[i]\n",
    "        score = score_list[i]\n",
    "        true_name = centroid_name_list[i]\n",
    "        size = size_list[i]\n",
    "        tweet_id = tweet_id_list[i]\n",
    "        author =  author_list[i]\n",
    "        db[col_output_event].update_one({'time':time,'description':description,'group_id':i},\n",
    "                          {\"$set\":{'centroid': centroid, \n",
    "                                   'name':name, \n",
    "                                   'score': score,\n",
    "                                   'true_name': true_name,\n",
    "                                   'size': size,\n",
    "                                   'tweet_id':tweet_id,\n",
    "                                   'past': [],\n",
    "                                   'future': [],\n",
    "                                  }}, upsert=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_id(sample_doc, group):\n",
    "    tweet_id_list = []\n",
    "    author_list = []\n",
    "    for member in group:\n",
    "        buf = []\n",
    "        buf2 = set()\n",
    "        for i in member:\n",
    "            tweet_id = sample_doc[i]['tweet_id']\n",
    "            author = sample_doc[i]['author']\n",
    "            buf.append(tweet_id)\n",
    "            buf2.add(author)\n",
    "        tweet_id_list.append(buf)\n",
    "        author_list.append(buf2)\n",
    "    return tweet_id_list, author_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python27\\Lib\\site-packages\\scipy\\spatial\\distance.py:505: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n",
      "[########################################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "bar = progressbar.ProgressBar(maxval=len(s_text)+1, widgets=[progressbar.Bar('#', '[', ']'), ' ', progressbar.Percentage()])\n",
    "bar.start()\n",
    "data=1\n",
    "\n",
    "for date in s_text.keys():\n",
    "    data += 1\n",
    "    bar.update(data)\n",
    "    \n",
    "    sample = s_text[date]\n",
    "    sample_doc = s_all_doc[date]\n",
    "    \n",
    "    if(len(sample) < 20):\n",
    "        continue\n",
    "    \n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
    "    tf = tf_vectorizer.fit_transform(sample)\n",
    "    feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    cosine_sim = cosine_similarity(tf)\n",
    "    cosine_dis = cosine_distances(tf)\n",
    "    \n",
    "    #  get labels_true \n",
    "    topic_labels, minor_class_labels, major_class_labels, topic_dense = get_labels_true(sample_doc)\n",
    "    \n",
    "    # DBSCAN  \n",
    "    db_group, db_labels, db_n_clusters = dbscan(cosine_dis)\n",
    "    \n",
    "    centroid_list, centroid_word_list, score_list, centroid_name_list, size_list = find_centroid_score(tf,db_group,feature_names,topic_labels)\n",
    "#     print type(centroid_list),centroid_list\n",
    "    tweet_id_list, author_list = get_tweet_id(sample_doc, db_group)\n",
    "    write_event(date,'DBSCAN', feature_names, centroid_list, centroid_word_list, score_list, centroid_name_list, size_list, tweet_id_list, author_list)\n",
    "    \n",
    "    \n",
    "    # Affinity Propagation  \n",
    "    aff_group, aff_labels, aff_n_clusters, aff_cluster_centers, aff_filter_group, aff_filter_labels, aff_filter_n_clusters, aff_filter_centers = aff_cluster(tf)\n",
    "    \n",
    "    centroid_list, centroid_word_list, score_list, centroid_name_list, size_list = find_centroid_score_aff(tf, aff_group, feature_names, topic_labels, aff_cluster_centers)\n",
    "#     print type(centroid_list),centroid_list\n",
    "    tweet_id_list, author_list = get_tweet_id(sample_doc, aff_group)\n",
    "    \n",
    "    write_event(date,'Affinity Propagation', feature_names, centroid_list, centroid_word_list, score_list, centroid_name_list, size_list, tweet_id_list, author_list)\n",
    "     \n",
    "    \n",
    "    # Markov Cluster    \n",
    "    mcl = run_mcl(cosine_sim ,inflation = 1.5)\n",
    "    mcl_cluster = get_clusters(mcl)\n",
    "    mcl_labels, mcl_filter_labels , mcl_filter_cluster= label_mcl(mcl_cluster, len(cosine_sim))\n",
    "    \n",
    "    \n",
    "    centroid_list, centroid_word_list, score_list, centroid_name_list, size_list = find_centroid_score(tf, mcl_cluster, feature_names, topic_labels)\n",
    "    tweet_id_list, author_list = get_tweet_id(sample_doc, mcl_cluster)\n",
    "    write_event(date,'Markov Cluster', feature_names, centroid_list, centroid_word_list, score_list, centroid_name_list, size_list, tweet_id_list, author_list)\n",
    "    \n",
    "    # cluster_score\n",
    "    levels = ['topic', 'minor_class', 'major_class']\n",
    "    labels_true = [topic_labels, minor_class_labels, major_class_labels]\n",
    "    description = ['DBSCAN','Affinity Propagation','Markov Cluster']\n",
    "    labels = [db_labels,aff_labels,mcl_labels]\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            cluster_score(date, levels[i], description[j], labels_true[i], labels[j],topic_dense)\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.827586206897\n",
      "0.832214765101\n"
     ]
    }
   ],
   "source": [
    "lt = [1,1,2,2,3,3,]\n",
    "l =  [1,1,1,2,3,3,]\n",
    "print rend_index(lt, l)\n",
    "\n",
    "lt = [1,1,2,2,3,3, 1,1,2,2,3,3, 1,1,2,2,3,3, 1,1,2,2,3,3, 1,1,2,2,3,3,]\n",
    "l =  [1,1,1,2,3,3, 1,1,1,2,3,3, 1,1,1,2,3,3, 1,1,1,2,3,3, 1,1,1,2,3,3,]\n",
    "print rend_index(lt, l)\n",
    "\n",
    "lt = [1,1,2,2,3,3, 1,1,2,2,3,3, 1,1,2,2,3,3, 1,1,2,2,3,3, 1,1,2,2,3,3,\n",
    "      1,1,2,2,3,3, 1,1,2,2,3,3, 1,1,2,2,3,3, 1,1,2,2,3,3, 1,1,2,2,3,3,\n",
    "      1,1,2,2,3,3, 1,1,2,2,3,3, 1,1,2,2,3,3, 1,1,2,2,3,3, 1,1,2,2,3,3,\n",
    "      1,1,2,2,3,3, 1,1,2,2,3,3, 1,1,2,2,3,3, 1,1,2,2,3,3, 1,1,2,2,3,3,\n",
    "      1,1,2,2,3,3, 1,1,2,2,3,3, 1,1,2,2,3,3, 1,1,2,2,3,3, 1,1,2,2,3,3,]\n",
    "l =  [1,1,1,2,3,3, 1,1,1,2,3,3, 1,1,1,2,3,3, 1,1,1,2,3,3, 1,1,1,2,3,3,\n",
    "      1,1,1,2,3,3, 1,1,1,2,3,3, 1,1,1,2,3,3, 1,1,1,2,3,3, 1,1,1,2,3,3,\n",
    "      1,1,1,2,3,3, 1,1,1,2,3,3, 1,1,1,2,3,3, 1,1,1,2,3,3, 1,1,1,2,3,3,\n",
    "      1,1,1,2,3,3, 1,1,1,2,3,3, 1,1,1,2,3,3, 1,1,1,2,3,3, 1,1,1,2,3,3,\n",
    "      1,1,1,2,3,3, 1,1,1,2,3,3, 1,1,1,2,3,3, 1,1,1,2,3,3, 1,1,1,2,3,3,]\n",
    "print rend_index(lt, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.833333333333\n",
      "0.833333333333\n"
     ]
    }
   ],
   "source": [
    "print (1.0/6)*(2+1+2)\n",
    "print (1.0/30)*(10+5+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-a7f0e0b59763>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0ms1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcal_cluster_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "lt = [1,1,1,2,2]\n",
    "l =  [1,1,2,2,2]\n",
    "print cal_cluster_score(lt, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
