{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python2\n",
    "# -*- coding: utf-8 -*-\n",
    "import pprint\n",
    "import pymongo\n",
    "import datetime\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from scipy import spatial\n",
    "from scipy.sparse import isspmatrix, dok_matrix, csc_matrix\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import pylab\n",
    "\n",
    "import os\n",
    "\n",
    "import progressbar\n",
    "\n",
    "db_name = 'twitter'\n",
    "col_name = 'after_process_replab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN, AffinityPropagation, MeanShift, estimate_bandwidth\n",
    "# from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "\n",
    "from igraph import *\n",
    "import igraph\n",
    "import math\n",
    "from operator import itemgetter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPrettyPrinter(pprint.PrettyPrinter):\n",
    "    def format(self, object, context, maxlevels, level):\n",
    "        if isinstance(object, unicode):\n",
    "            return (object.encode('thai'), True, False)\n",
    "        return pprint.PrettyPrinter.format(self, object, context, maxlevels, level)\n",
    "\n",
    "def go_print( input ):\n",
    "    MyPrettyPrinter().pprint(input)\n",
    "    # ppp = pprint.PrettyPrinter(indent=4)\n",
    "    # ppp.pprint(input)\n",
    "    return;\n",
    "\n",
    "def get_midnight(time):\n",
    "    return time.replace(minute=0, hour=0, second=0, microsecond=0)\n",
    "\n",
    "def get_time_gap(time,hour_gap=1,min_gap=1):\n",
    "    h = time.hour\n",
    "    m = time.minute\n",
    "    o_h = h/hour_gap*hour_gap\n",
    "    o_m = m/min_gap*min_gap\n",
    "    return time.replace(hour=o_h, minute=o_m, second=0, microsecond=0)\n",
    "\n",
    "def get_week_year(time):\n",
    "    return tuple([time.isocalendar()[0], time.isocalendar()[1]])\n",
    "\n",
    "def get_thai_midnight(time):\n",
    "    out = time + datetime.timedelta(hours=7)\n",
    "    out = out.replace(minute=0, hour=0, second=0, microsecond=0) - datetime.timedelta(hours=7)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim(doc_a,doc_b):\n",
    "    return 1 - spatial.distance.cosine(doc_a, doc_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a = [1, 1, 0, 0 ,0, 2]\n",
    "b = [0, 0, 1, 2, 0, 0]\n",
    "print cosine_similarity([a], [b])\n",
    "print get_sim(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    client = MongoClient()\n",
    "    db = client[db_name]\n",
    "\n",
    "    result = db[col_name].create_index([('ts', pymongo.ASCENDING)])\n",
    "    cursor = db[col_name].find({}).sort([('ts', pymongo.ASCENDING)])\n",
    "    \n",
    "\n",
    "    date = []\n",
    "    text = []\n",
    "    all_doc = []\n",
    "\n",
    "    bar = progressbar.ProgressBar(maxval=cursor.count()+1, widgets=[progressbar.Bar('#', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    data=1\n",
    "    for doc in cursor:\n",
    "        data += 1\n",
    "        bar.update(data)\n",
    "        \n",
    "        ts = doc['ts']\n",
    "        datetime_object = datetime.datetime.fromtimestamp(ts)\n",
    "        if True:\n",
    "#         if len(doc['hashtags']) != 0:\n",
    "            buf = doc['nouns_nltk']\n",
    "            for x in doc['hashtags']:\n",
    "                if not x.lower() in buf:\n",
    "                    buf.append(x.lower())\n",
    "            all_doc.append(doc)\n",
    "            date.append(datetime_object)\n",
    "            text.append(' '.join(buf))\n",
    "    bar.finish()\n",
    "#     print len(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_group(n_cluster, all_doc, doc_key, g_id):\n",
    "        group = []\n",
    "        for i in range(n_cluster):\n",
    "            group.append([])\n",
    "        \n",
    "        for i in range( len(g_id) ):\n",
    "            group[ g_id[i] ].append(all_doc[i][doc_key])\n",
    "        return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_day(date,text,all_doc):\n",
    "    s_text = {}\n",
    "    s_all_doc = {}\n",
    "    for datetime_object in date:\n",
    "        i = date.index(datetime_object)\n",
    "        key = get_midnight(datetime_object)\n",
    "        if not s_text.has_key(key):\n",
    "            s_text[key] = []\n",
    "            s_all_doc[key] = []\n",
    "        s_text[key].append(text[i])\n",
    "        s_all_doc[key].append(all_doc[i])\n",
    "    return s_text,s_all_doc\n",
    "\n",
    "def split_week(date,text,all_doc):\n",
    "    s_text = {}\n",
    "    s_all_doc = {}\n",
    "    for datetime_object in date:\n",
    "        i = date.index(datetime_object)\n",
    "        key = get_week_year(datetime_object)\n",
    "        if not s_text.has_key(key):\n",
    "            s_text[key] = []\n",
    "            s_all_doc[key] = []\n",
    "        s_text[key].append(text[i])\n",
    "        s_all_doc[key].append(all_doc[i])\n",
    "    return s_text,s_all_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_text,s_all_doc = split_day(date,text,all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print s_text.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 3\n",
    "sample = []\n",
    "ts = 1349689576\n",
    "datetime_o = datetime.datetime.fromtimestamp(ts)\n",
    "k = get_midnight(datetime_o)\n",
    "\n",
    "sample = s_text[k]\n",
    "sample_doc = s_all_doc[k]\n",
    "\n",
    "# print sample\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
    "tf = tf_vectorizer.fit_transform(sample)\n",
    "feature_names = tf_vectorizer.get_feature_names()\n",
    "print feature_names\n",
    "# print '\\n\\n tf-idf'\n",
    "# tfidf_vectorizer_2 = TfidfVectorizer(max_df=0.95, min_df=2)\n",
    "# tf2 = tfidf_vectorizer_2.fit_transform(sample)\n",
    "# print tf2\n",
    "test = cosine_similarity(tf)\n",
    "test2 = cosine_distances(tf)\n",
    "\n",
    "# for sim in test[0]:\n",
    "#     if sim >= 0.1:\n",
    "#         print sim, test[0].tolist().index(sim)\n",
    "# print len(test)\n",
    "\n",
    "print test[0]\n",
    "print test2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print tf[0]\n",
    "# print feature_names[165]\n",
    "print sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print test[0]\n",
    "# test2 = StandardScaler().fit_transform(test)\n",
    "# print test2[0]\n",
    "print len(test2)\n",
    "# print test2[479]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_centroid_score(tf,group_member,feature_names):\n",
    "    centroid_list = []\n",
    "    centroid_word_list = []\n",
    "    score_list = []\n",
    "    i = 0\n",
    "    for group in group_member:\n",
    "        count = np.zeros(len(feature_names))\n",
    "        for member in group:\n",
    "            count += tf[member].toarray()[0]\n",
    "        centroid = count / len(group)\n",
    "        centroid_list.append(centroid)\n",
    "        \n",
    "        centroid_word = []\n",
    "        for word_index in range(len(feature_names)):\n",
    "            if count[word_index] != 0 and centroid[word_index] > 0.5:\n",
    "                word_data = '%.5f' % centroid[word_index] +' '+str(feature_names[word_index])\n",
    "                centroid_word.append(word_data)\n",
    "        centroid_word_list.append( sorted(centroid_word, reverse  = True))\n",
    "        print i,':',sorted(centroid_word, reverse  = True)\n",
    "                \n",
    "        sum_sim = 0\n",
    "        for member in group:\n",
    "            m_tf = tf[member].toarray()[0]\n",
    "            sum_sim += get_sim(m_tf,centroid)\n",
    "        score = sum_sim/len(group)\n",
    "        score_list.append(score)\n",
    "        \n",
    "        i +=1\n",
    "    return centroid_list, centroid_word_list, score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dbscan(test):\n",
    "    # Compute DBSCAN\n",
    "    cluster = DBSCAN(eps=0.3, min_samples=3,metric='precomputed').fit(test)\n",
    "    # distance <= eps\n",
    "    core_samples_mask = np.zeros_like(cluster.labels_, dtype=bool)\n",
    "    core_samples_mask[cluster.core_sample_indices_] = True\n",
    "    labels = cluster.labels_\n",
    "    \n",
    "#     print cluster.core_sample_indices_\n",
    "\n",
    "    print labels\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "    print('Estimated number of clusters: %d' % n_clusters_)\n",
    "    if n_clusters_>1:\n",
    "        print(\"Silhouette Coefficient: %0.3f\"\n",
    "          % metrics.silhouette_score(test, labels))\n",
    "        \n",
    "    group = []\n",
    "    for i in range(n_clusters_+1):\n",
    "        group.append([])\n",
    "    index = 0\n",
    "    for i in labels:\n",
    "        group[i].append(index)\n",
    "        index += 1\n",
    "    \n",
    "    group = group[:-1]\n",
    "    # for i in range(n_clusters_):\n",
    "    #     print len(group[i])\n",
    "\n",
    "    return group, labels, n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group, labels, n_clusters_ = dbscan(test2)\n",
    "i = 0\n",
    "for mem in group:\n",
    "    print i,mem\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group_2 = view_group(n_clusters_+1 , sample_doc, \"topic\", labels)\n",
    "group_2 = group_2[:-1]\n",
    "x = 0\n",
    "for i in group_2:\n",
    "    print x,':',i\n",
    "    x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_list, centroid_word_list, score_list = find_centroid_score(tf,group,feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AffinityPropagation\n",
    "def aff_cluster(test):\n",
    "    # Compute DBSCAN\n",
    "    cluster = AffinityPropagation(affinity='euclidean').fit(test)\n",
    "    cluster_centers_indices = cluster.cluster_centers_indices_\n",
    "    labels = cluster.labels_\n",
    "    print cluster_centers_indices\n",
    "#     print labels\n",
    "\n",
    "    # print labels\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels))\n",
    "#     print('Estimated number of clusters: %d' % n_clusters_)\n",
    "        \n",
    "    group = []\n",
    "    for i in range(n_clusters_):\n",
    "        group.append([])\n",
    "    index = 0\n",
    "    for i in labels:\n",
    "        group[i].append(index)\n",
    "        index += 1\n",
    "        \n",
    "    regroup = []\n",
    "    relabel = []\n",
    "    index = 0\n",
    "    for mem in group:\n",
    "        if len(mem) >= 3:\n",
    "            regroup.append(mem)\n",
    "            relabel.append(index)\n",
    "#             print index\n",
    "        index += 1\n",
    "    \n",
    "    new_label = []\n",
    "    for i in labels:\n",
    "        if i in relabel:\n",
    "            new_label.append(relabel.index(i))\n",
    "        else:\n",
    "            new_label.append(-1)\n",
    "    # for i in range(n_clusters_):\n",
    "    #     print len(group[i])\n",
    "    print new_label\n",
    "    n_clusters_ = len(set(new_label))\n",
    "    print('Estimated number of clusters: %d' % n_clusters_)\n",
    "\n",
    "    return regroup, new_label, n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group, labels, n_clusters_ = aff_cluster(tf)\n",
    "i = 0\n",
    "for mem in group:\n",
    "    print i,mem\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_2 = view_group(n_clusters_ , sample_doc, \"topic\", labels)\n",
    "x = 0\n",
    "for i in group_2:\n",
    "    print x,':',i\n",
    "    x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_list, centroid_word_list, score_list = find_centroid_score(tf,group,feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_allclose(a, b, rtol=1e-5, atol=1e-8):\n",
    "    \"\"\"\n",
    "    Version of np.allclose for use with sparse matrices\n",
    "    \"\"\"\n",
    "    c = np.abs(a - b) - rtol * np.abs(b)\n",
    "    # noinspection PyUnresolvedReferences\n",
    "    return c.max() <= atol\n",
    "\n",
    "\n",
    "def normalize(matrix):\n",
    "    \"\"\"\n",
    "    Normalize the columns of the given matrix\n",
    "    \n",
    "    :param matrix: The matrix to be normalized\n",
    "    :returns: The normalized matrix\n",
    "    \"\"\"\n",
    "    return sklearn.preprocessing.normalize(matrix, norm=\"l1\", axis=0)\n",
    "\n",
    "\n",
    "def inflate(matrix, power):\n",
    "    \"\"\"\n",
    "    Apply cluster inflation to the given matrix by raising\n",
    "    each element to the given power.\n",
    "    \n",
    "    :param matrix: The matrix to be inflated\n",
    "    :param power: Cluster inflation parameter\n",
    "    :returns: The inflated matrix\n",
    "    \"\"\"\n",
    "    if isspmatrix(matrix):\n",
    "        return normalize(matrix.power(power))\n",
    "\n",
    "    return normalize(np.power(matrix, power))\n",
    "\n",
    "\n",
    "def expand(matrix, power):\n",
    "    \"\"\"\n",
    "    Apply cluster expansion to the given matrix by raising\n",
    "    the matrix to the given power.\n",
    "    \n",
    "    :param matrix: The matrix to be expanded\n",
    "    :param power: Cluster expansion parameter\n",
    "    :returns: The expanded matrix\n",
    "    \"\"\"\n",
    "    if isspmatrix(matrix):\n",
    "        return matrix ** power\n",
    "\n",
    "    return np.linalg.matrix_power(matrix, power)\n",
    "\n",
    "\n",
    "def add_self_loops(matrix, loop_value):\n",
    "    \"\"\"\n",
    "    Add self-loops to the matrix by setting the diagonal\n",
    "    to loop_value\n",
    "    \n",
    "    :param matrix: The matrix to add loops to\n",
    "    :param loop_value: Value to use for self-loops\n",
    "    :returns: The matrix with self-loops\n",
    "    \"\"\"\n",
    "    shape = matrix.shape\n",
    "    assert shape[0] == shape[1], \"Error, matrix is not square\"\n",
    "\n",
    "    if isspmatrix(matrix):\n",
    "        new_matrix = matrix.todok()\n",
    "    else:\n",
    "        new_matrix = matrix.copy()\n",
    "\n",
    "    for i in range(shape[0]):\n",
    "        new_matrix[i, i] = loop_value\n",
    "\n",
    "    if isspmatrix(matrix):\n",
    "        return new_matrix.tocsc()\n",
    "\n",
    "    return new_matrix\n",
    "\n",
    "\n",
    "def prune(matrix, threshold):\n",
    "    \"\"\"\n",
    "    Prune the matrix so that very small edges are removed\n",
    "    \n",
    "    :param matrix: The matrix to be pruned\n",
    "    :param threshold: The value below which edges will be removed\n",
    "    :returns: The pruned matrix\n",
    "    \"\"\"\n",
    "    if isspmatrix(matrix):\n",
    "        pruned = dok_matrix(matrix.shape)\n",
    "        pruned[matrix >= threshold] = matrix[matrix >= threshold]\n",
    "        pruned = pruned.tocsc()\n",
    "    else:\n",
    "        pruned = matrix.copy()\n",
    "        pruned[pruned < threshold] = 0\n",
    "\n",
    "    return pruned\n",
    "\n",
    "\n",
    "def converged(matrix1, matrix2):\n",
    "    \"\"\"\n",
    "    Check for convergence by determining if \n",
    "    matrix1 and matrix2 are approximately equal.\n",
    "    \n",
    "    :param matrix1: The matrix to compare with matrix2\n",
    "    :param matrix2: The matrix to compare with matrix1\n",
    "    :returns: True if matrix1 and matrix2 approximately equal\n",
    "    \"\"\"\n",
    "    if isspmatrix(matrix1) or isspmatrix(matrix2):\n",
    "        return sparse_allclose(matrix1, matrix2)\n",
    "\n",
    "    return np.allclose(matrix1, matrix2)\n",
    "\n",
    "\n",
    "def iterate(matrix, expansion, inflation):\n",
    "    \"\"\"\n",
    "    Run a single iteration (expansion + inflation) of the mcl algorithm\n",
    "    \n",
    "    :param matrix: The matrix to perform the iteration on\n",
    "    :param expansion: Cluster expansion factor\n",
    "    :param inflation: Cluster inflation factor\n",
    "    \"\"\"\n",
    "    # Expansion\n",
    "    matrix = expand(matrix, expansion)\n",
    "\n",
    "    # Inflation\n",
    "    matrix = inflate(matrix, inflation)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def get_clusters(matrix):\n",
    "    \"\"\"\n",
    "    Retrieve the clusters from the matrix\n",
    "    \n",
    "    :param matrix: The matrix produced by the MCL algorithm\n",
    "    :returns: A list of tuples where each tuple represents a cluster and\n",
    "              contains the indices of the nodes belonging to the cluster\n",
    "    \"\"\"\n",
    "    if not isspmatrix(matrix):\n",
    "        # cast to sparse so that we don't need to handle different \n",
    "        # matrix types\n",
    "        matrix = csc_matrix(matrix)\n",
    "\n",
    "    # get the attractors - non-zero elements of the matrix diagonal\n",
    "    attractors = matrix.diagonal().nonzero()[0]\n",
    "\n",
    "    # somewhere to put the clusters\n",
    "    clusters = set()\n",
    "\n",
    "    # the nodes in the same row as each attractor form a cluster\n",
    "    for attractor in attractors:\n",
    "        cluster = tuple(matrix.getrow(attractor).nonzero()[1].tolist())\n",
    "        clusters.add(cluster)\n",
    "\n",
    "    return sorted(list(clusters))\n",
    "\n",
    "\n",
    "def run_mcl(matrix, expansion=2, inflation=2, loop_value=1,\n",
    "            iterations=100, pruning_threshold=0.001, pruning_frequency=1,\n",
    "            convergence_check_frequency=1, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform MCL on the given similarity matrix\n",
    "    \n",
    "    :param matrix: The similarity matrix to cluster\n",
    "    :param expansion: The cluster expansion factor\n",
    "    :param inflation: The cluster inflation factor\n",
    "    :param loop_value: Initialization value for self-loops\n",
    "    :param iterations: Maximum number of iterations\n",
    "           (actual number of iterations will be less if convergence is reached)\n",
    "    :param pruning_threshold: Threshold below which matrix elements will be set\n",
    "           set to 0\n",
    "    :param pruning_frequency: Perform pruning every 'pruning_frequency'\n",
    "           iterations. \n",
    "    :param convergence_check_frequency: Perform the check for convergence\n",
    "           every convergence_check_frequency iterations\n",
    "    :param verbose: Print extra information to the console\n",
    "    :returns: The final matrix\n",
    "    \"\"\"\n",
    "    print(\"-\" * 50)\n",
    "    print(\"MCL Parameters\")\n",
    "    print(\"Expansion: {}\".format(expansion))\n",
    "    print(\"Inflation: {}\".format(inflation))\n",
    "    if pruning_threshold > 0:\n",
    "        print(\"Pruning threshold: {}, frequency: {} iteration{}\".format(\n",
    "            pruning_threshold, pruning_frequency, \"s\" if pruning_frequency > 1 else \"\"))\n",
    "    else:\n",
    "        print(\"No pruning\")\n",
    "    print(\"Convergence check: {} iteration{}\".format(\n",
    "        convergence_check_frequency, \"s\" if convergence_check_frequency > 1 else \"\"))\n",
    "    print(\"Maximum iterations: {}\".format(iterations))\n",
    "    print(\"{} matrix mode\".format(\"Sparse\" if isspmatrix(matrix) else \"Dense\"))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Initialize self-loops\n",
    "    if loop_value > 0:\n",
    "        matrix = add_self_loops(matrix, loop_value)\n",
    "\n",
    "    # Normalize\n",
    "    matrix = normalize(matrix)\n",
    "\n",
    "    # iterations\n",
    "    for i in range(iterations):\n",
    "        print(\"Iteration {}\".format(i + 1))\n",
    "\n",
    "        # store current matrix for convergence checking\n",
    "        last_mat = matrix.copy()\n",
    "\n",
    "        # perform MCL expansion and inflation\n",
    "        matrix = iterate(matrix, expansion, inflation)\n",
    "\n",
    "        # prune\n",
    "        if pruning_threshold > 0 and i % pruning_frequency == pruning_frequency - 1:\n",
    "            print(\"Pruning\")\n",
    "            matrix = prune(matrix, pruning_threshold)\n",
    "\n",
    "        # Check for convergence\n",
    "        if i % convergence_check_frequency == convergence_check_frequency - 1:\n",
    "            print(\"Checking for convergence\")\n",
    "            if converged(matrix, last_mat):\n",
    "                print(\"Converged after {} iteration{}\".format(i + 1, \"s\" if i > 0 else \"\"))\n",
    "                break\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mcl = run_mcl(test,inflation = 1.5)\n",
    "cluster = get_clusters(mcl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for py 3.xx \n",
    "# import markov_clustering as mc\n",
    "\n",
    "# mcl = mc.run_mcl(test,inflation = 1.5)           # run MCL with default parameters\n",
    "# cluster = mc.get_clusters(mcl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "labels = [0]*len(test)\n",
    "g_index = 0\n",
    "for mem in cluster:\n",
    "    for i in mem:\n",
    "        labels[i] = g_index\n",
    "    g_index += 1\n",
    "n_clusters_ = len(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_2 = view_group(n_clusters_ , sample_doc, \"topic\", labels)\n",
    "x = 0\n",
    "for i in group_2:\n",
    "    print x,':',i\n",
    "    x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_list, centroid_word_list, score_list = find_centroid_score(tf,group,feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_eps(test):\n",
    "    eps_set = []\n",
    "    n = len(test)\n",
    "    k = min(2,n-1)\n",
    "    for i in range(n):\n",
    "        eps_set.append(sorted(test[i])[2])\n",
    "        \n",
    "    return eps_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = find_eps(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,1,2,2,3,3]\n",
    "b = [1,1,3,3,2,2]\n",
    "metrics.adjusted_rand_score(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
